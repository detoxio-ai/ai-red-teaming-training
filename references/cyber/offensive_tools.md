# Open-Source LLM and GenAI Tools for Offensive Security

Advances in large language models (LLMs) and generative AI have spawned a new class of offensive security tools. These open-source projects leverage AI for tasks ranging from reconnaissance and fuzzing to vulnerability discovery and exploitation. Below we analyze several popular tools designed for red team and penetration testing scenarios, detailing what they do, how they use LLMs/AI, their use cases, supported models, and their licensing and community status.

## PentestGPT (GitHub: GreyDGL/PentestGPT)

**Description:** PentestGPT is an open-source penetration testing assistant powered by LLMs. It runs as a command-line tool that interactively guides a tester through the pen-test process. PentestGPT feels like a terminal-based “junior pentester,” offering step-by-step suggestions and reasoning to complement human expertise.

**LLM Integration:** The tool leverages GPT models (preferably OpenAI’s GPT-4) to analyze contexts and generate next-step commands or payloads. It implements a **tripartite architecture** – a Reasoning module (high-level strategy), Generation module (producing specific commands), and Parsing module (interpreting tool outputs) – modeled after a collaborative team of lead and junior testers. This design helps maintain context and “test status awareness” better than using a raw GPT-4 chat alone. Essentially, PentestGPT uses GPT-4’s natural language understanding to maintain a testing plan (in a Pentesting Task Tree) and to synthesize or explain commands and results as the engagement progresses.

**Use Cases:** PentestGPT supports a wide range of offensive tasks: it can guide reconnaissance (e.g. suggesting how to enumerate targets), interpret scanner output or source code to identify vulnerabilities, suggest exploits or next actions, and even assist with report generation by explaining findings. Notably, it doesn’t execute actions automatically – the human runs actual tools – but it provides the logic and commands to use. It has demonstrated success on platforms like HackTheBox (solving 4 of 10 challenges) and CTFs by reasoning through multi-step exploits.

**Supported Models & Platforms:** PentestGPT is model-agnostic but was built with OpenAI APIs in mind. By default it uses *GPT-4* (denoted “gpt-4o”) for reasoning, with fallback to GPT-3.5 for less critical tasks. It also supports certain local/offline LLMs via community integrations – for example, GPT4All or similar local models can be configured for air-gapped environments. The tool is written in Python and runs on any system with Python 3.10+, requiring an API key for the chosen LLM backend.

**Licensing & Community:** PentestGPT is released under the MIT License. It has a vibrant community – the project’s GitHub has garnered over 6,000 stars by mid-2025 with active contributors. The authors continue to update the tool (planning v1.0 and a successor project) and caution users to use the official open-source version (there have been scam copycats given its popularity). This strong community interest and ongoing research (USENIX Security 2024 paper) underscore PentestGPT’s impact in the offensive AI space.

## Nebula (GitHub: berylliumsec/nebula)

**Description:** Nebula is an AI-powered penetration testing assistant developed by Beryllium Security. It acts as a smart shell environment for pentesters, automating reconnaissance, note-taking, and vulnerability analysis tasks. Nebula runs either in a terminal UI or with an optional GUI, and it integrates tightly with common security tools to create a seamless workflow.

**LLM Integration:** Unlike tools that rely on cloud APIs, Nebula packages **open-source LLMs** to run locally. On first run it downloads models such as *Llama 3.1 8B*, *Mistral 7B Instruct*, or *DeepSeek-Llama 8B* and uses them for NLP tasks. The AI is invoked by a simple prefix in the terminal (e.g. typing `!` before a query). Nebula’s LLM interprets user natural-language commands (e.g. “find SQL injection vulnerabilities on the target”) and translates them into sequences of real tool commands, effectively bridging human intent to actual tool execution. It also parses tool output in real-time – for example, if `nmap` finds open ports, Nebula’s AI can suggest next steps or highlight interesting findings from the scan results. In “Autonomous Mode” (in development), Nebula aims to chain tool outputs to next actions without user prompt, gradually moving toward fully AI-driven pentesting.

**Use Cases:** Nebula focuses on *reconnaissance and analysis* efficiency. It can run any CLI-based scanner or exploit tool and then leverage AI to: provide context-aware recommendations, summarize results, log findings, and even assist in report writing. For example, it can automate web recon (with tools like Nmap, OWASP ZAP, subdomain finders, etc. integrated) and use AI to identify likely vulnerabilities or generate exploit payloads. All commands and outputs are logged and organized by Nebula, which helps with note-taking and later reporting. This makes it useful for lengthy engagements where keeping track of discovered information is challenging. Nebula does **not** replace security tools but augments them – the AI might suggest “try SQLMap on parameter X” or auto-generate a payload, but it will call the actual `sqlmap` tool to execute it.

**Supported Models & Platforms:** Nebula emphasizes privacy by running models locally. It uses the [Ollama](https://github.com/jmorganca/ollama) backend to serve models on the user’s machine and can work without Internet once models are downloaded. Models tested include Llama2-based and Mistral models optimized for instruct tasks. This means Nebula can run entirely offline on a reasonably powerful machine (the models are 7–8B parameters, requiring several GB of RAM). Nebula is cross-platform (primarily Linux/macOS) and integrates with any command-line tool (it comes pre-configured with tools like Nmap, CrackMapExec, Nuclei, etc., but users can add others).

**Licensing & Community:** The project is open-source under a BSD-2-Clause license. It is a newer initiative (v2.0 released in 2025) and has a growing community (hundreds of stars on GitHub). Nebula is actively maintained with regular updates and an upcoming “Nebula Pro” extension (for more autonomous operation). Its development indicates an emphasis on collaboration – BerylliumSec also open-sourced a related tool “Neutron” and provides community support. Nebula’s traction shows in its inclusion in multiple “top AI pentesting tools” lists.

## ChatAFL (GitHub: ChatAFLndss/ChatAFL)

**Description:** ChatAFL is a research prototype that **uses LLM guidance to enhance protocol fuzzing**. Developed by academic researchers (published at NDSS 2024), it builds on AFLNet (a fuzzing engine for network protocols) and integrates an LLM to make fuzzing smarter. The goal is to fuzz network services (especially text-based protocols) more effectively by leveraging the language model’s knowledge of protocol structure and semantics.

**LLM Integration:** In ChatAFL’s design, the LLM is employed in three main ways during fuzzing: (1) **Grammar Inference** – The LLM is prompted with examples or documentation of the protocol to produce a machine-readable grammar for message formats. This grammar helps AFLNet do structure-aware mutations instead of random bit flips. (2) **Seed Generation** – The LLM generates diverse initial test messages to seed the fuzzer, expanding coverage of different protocol features. (For instance, given a partial HTTP request, the LLM might suggest additional header combinations or unusual values as starting points.) (3) **Breaking Plateaus** – If the fuzzer gets stuck (no new coverage for a while), ChatAFL can ask the LLM to craft a new input that might exercise unseen code paths, essentially brainstorming a next-step in the protocol dialogue. These LLM-crafted inputs are then fed back into the fuzzing loop. The LLM (such as GPT-3.5/4 via OpenAI API) acts as an on-demand generator of intelligent test cases, guided by coverage feedback.

**Use Cases:** ChatAFL is tailored for *network protocol vulnerability discovery*. Offensive security researchers can use it to test servers speaking protocols like HTTP, SMTP, FTP, or custom text-based protocols. By understanding protocol context, the fuzzer can, for example, maintain valid message sequences (so it can get past handshake stages) and then mutate specific fields to hunt for flaws like buffer overflows or logic bugs. This approach is especially useful for complex, stateful protocols where dumb fuzzing would often send irrelevant gibberish. In practice, using ChatAFL involves setting up a target service in a Docker environment and providing an OpenAI API key; the framework then runs automated fuzz campaigns. The output is similar to AFL’s results – crashes or hangs identified – but ideally with higher coverage thanks to the LLM’s input guidance. This tool is more for advanced red teamers or researchers aiming to uncover 0-day vulnerabilities in protocol implementations.

**Supported Models & Platforms:** The published ChatAFL prototype uses the OpenAI API (GPT-3.5 or GPT-4) for its intelligence. It’s implemented as a set of scripts around AFLNet, and the whole setup is distributed as Docker containers for reproducibility. This means it can run on Linux hosts (Docker required) and targets text-based network servers (the NDSS artifact included benchmarks like an HTTP server, FTP server, etc. in ProFuzzBench format). While not a plug-and-play tool for everyday use, it provides a template for how generative models can assist in fuzzing. ChatAFL is open-source (Apache-2.0 licensed) and can be extended to other LLMs or domains with some development effort.

**Licensing & Community:** Licensed under Apache 2.0, ChatAFL’s code is available on GitHub. It has a modest community – a few dozen forks and \~300+ stars, mostly comprised of academic and security research enthusiasts. As a research artifact, it may not have frequent updates outside of the authors’ contributions, but it stands as an example of LLM-enhanced fuzzing. The paper and code have been well-received in the fuzzing community, and it won a Distinguished Paper award at NDSS 2024, suggesting the techniques may influence future fuzzing tools.

## DeepExploit (GitHub: 13o-bbr-bbq/machine\_learning\_security *or* TheDreamPort/deep\_exploit)

**Description:** DeepExploit is a fully automated penetration testing tool that uses deep reinforcement learning (DRL) to find and exploit vulnerabilities. Developed around 2018 by Isao Takaesu and others, it was one of the earliest attempts to apply AI to offensive security. DeepExploit links with the Metasploit framework’s RPC API to run exploitation modules autonomously. Its innovation was to treat the sequence of scanning and exploiting as a reinforcement learning problem – the agent learns which actions (scans, exploits, pivoting, etc.) lead to successful compromise of target systems.

**AI Integration:** Instead of an LLM, DeepExploit uses a **generative Deep RL algorithm** (specifically A3C – Asynchronous Advantage Actor-Critic) to make decisions. The agent starts by scanning the target network (e.g., port scan, service enumeration) and based on findings, it picks an exploit from Metasploit to attempt. If exploitation succeeds, it can then pivot (update its state to include the newly compromised host and scan further). The RL agent’s “state” includes information like open ports, OS type, known vulnerabilities, etc., and the “reward” is higher for actions that lead to successful exploitation or deeper penetration. Through training (even without prior knowledge of the network), the agent improves its strategy over time. This self-learning capability means it can theoretically adapt to different environments and discover multi-step attack paths that a static script might miss. DeepExploit also employs some machine learning in target analysis – for example, to decide which exploit is likely to work, it might use simple ML classifiers or heuristics on the gathered data (though much of that logic was rules-based). The highlight is the agent’s ability to **learn from experience** – it reduces failed exploit attempts as it trains, achieving what the authors call “pinpointed execution” (finding a working exploit in as little as one attempt for some cases).

**Use Cases:** DeepExploit is designed for *autonomous network penetration testing*. In a red team scenario, one could unleash DeepExploit on an internal network segment; it will scan for hosts, identify potential vulnerabilities (e.g., via service banners or known CVEs), and then automatically try exploits. If it pops a shell on one machine, it then continues from there to find further targets (lateral movement). This makes it useful for thoroughly checking a network for exploitable weaknesses with minimal human input. It covers both **perimeter** (external services) and **internal** hosts by pivoting after initial compromise. However, as a DRL-based tool, it requires a controlled environment to train effectively and might generate some false positives or crash services in the process. DeepExploit outputs a report (including an HTML report of found vulnerabilities/exploits). It essentially automates what a human pentester would do with Nmap and Metasploit, but with an AI brain choosing the sequence of actions.

**Supported Models & Platforms:** The original DeepExploit code is written in Python and interacts with Metasploit (which typically runs on a Kali Linux host). It’s meant for *client-server* OS like Linux/Windows networks. The DRL neural network uses TensorFlow. Because it’s a somewhat experimental tool, environment setup can be tricky (some community forks have tried to update it for Python3 and newer Metasploit APIs). The “model” here refers to the DRL policy model, which the tool trains as it runs (the user doesn’t have to supply a pretrained model; it learns on the fly or can load a saved policy). There are also related projects like **AutoPentest-DRL** (from JAIST) that extended or reimplemented this concept with Deep Q-Learning, indicating an active research interest in DRL for pentesting.

**Licensing & Community:** DeepExploit’s code was open-sourced (it appears under a repository by the handle 13o-bbr-bbq, likely under Apache or MIT License). The project gained attention after being presented at Black Hat Asia 2018 Arsenal and DEF CON Demo Labs. On GitHub, the original code and its forks have on the order of a few hundred stars (for instance, a fork by TheDreamPort has updated code). While not as widely maintained today, it’s frequently cited as a milestone in offensive AI. The community around it is mostly researchers continuing experiments in automated exploitation. In practice, newer LLM-based approaches (like PentestGPT or AI agents) have somewhat overshadowed DeepExploit, but it remains an important reference for using generative *learning* (not just static knowledge) to conduct exploits autonomously.

## GyoiThon (GitHub: gyoisamurai/GyoiThon)

**Description:** GyoiThon is an automated web penetration testing tool that uses machine learning to assist in intelligence gathering and vulnerability assessment. Developed by a security team in Japan (MBSD), it was introduced in 2018 as a “Next generation pentest tool” combining multiple recon techniques with an ML backend. GyoiThon can scan a target web server to identify the software in use (e.g. CMS type, versions, frameworks) and then automatically launch Metasploit exploits for any known vulnerabilities in those products.

**AI Integration:** The AI in GyoiThon is primarily a **machine learning model for pattern matching** on web content. As the tool crawls the target site and collects data (server banners, HTML pages, error messages, default files, etc.), it applies string pattern matching and ML classification to infer what technologies are running and if there are known CVEs associated. For example, by analyzing page content and responses, GyoiThon’s ML engine might recognize a WordPress site of a certain version and flag the CVEs affecting that version. It also uses NLP techniques to parse things like HTML comments or error leaks for clues (like version numbers or debug modes). This intelligence is then used to choose relevant exploits. The “learning” aspect in GyoiThon includes a trained model (possibly a simple neural network or clustering algorithm) to improve identification of products/CVEs beyond what manual regexes can do. It’s not an LLM, but a narrower ML model tailored to web fingerprinting. Once products and possible vulns are identified, GyoiThon programmatically invokes Metasploit modules corresponding to those vulns, effectively automating the exploit step. It can also perform a rudimentary “health check” mode – a non-destructive scan to assess if vulnerabilities likely exist without fully exploiting them.

**Use Cases:** GyoiThon is suited for *web application reconnaissance and exploitation*. It automates what a web pentester might do in the early phases: gather subdomains, crawl pages, identify software (Apache, PHP, Joomla, etc.), then check those against a vulnerability database. Its ML-driven identification can sometimes uncover less obvious clues (e.g., a unique HTML comment indicating a specific plugin version). Offensive teams can use GyoiThon to quickly enumerate a target’s attack surface and even get actual shells if known exploits are available, all in one sweep. The tool integrates sources like Google Custom Search and Censys API for broader recon (finding additional subdomains or public info). It’s particularly useful in engagements where time is short – one command can perform hours’ worth of scanning and basic exploitation automatically. However, it’s limited to *known* vulnerabilities (it’s not going to find a brand-new 0-day via ML, rather it’s leveraging CVE databases), and the Metasploit exploitation is only as good as the modules available.

**Supported Models & Platforms:** GyoiThon is written in Python and intended for Kali Linux or similar environments with Metasploit installed. It requires some configuration (API keys for Google, Censys if used). The ML model is included in the repo (likely a pre-trained data set mapping web fingerprints to products/CVEs). GyoiThon uses Apache 2.0 licensed code and can be extended by updating its signatures or training data. It supports scanning both HTTP and HTTPS, does WHOIS lookups, and other typical recon tasks in an automated pipeline.

**Licensing & Community:** The project is Apache-2.0 licensed and was released publicly after being shown at multiple security conferences (JANOG41, Black Hat Arsenal, DEF CON Demo Labs in 2018). On GitHub it has around 700–800 stars, reflecting moderate interest. It’s a bit older and doesn’t see frequent updates now. Still, GyoiThon is often referenced as an early example of “AI for pentesting” in literature. Its community usage today may be limited, but it inspired later tools. Security professionals who tried GyoiThon noted that while it can automate common checks, it’s not a replacement for expert analysis – rather it offloads the grunt work of scanning and exploit testing. It remains a free tool that can enhance efficiency during web app tests, especially for finding low-hanging fruit.

## Summary Comparison

The table below compares the key features of each tool:

| **Tool** (GitHub) | **AI/LLM Usage**                                                                                                                                                                                                                                              | **Offensive Use Cases**                                                                                                                                                                                                                                 | **Supported Models**                                                                                                                                                                                   | **License**                                                                     | **Community & Popularity**                                                                                                                                                                                                  |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **PentestGPT**    | GPT-3.5/4 powered assistant; uses LLM for reasoning, command generation, and output parsing. Maintains context via a task-tree to guide pentesters step-by-step.                                                                                              | Guided pentesting workflow: recon suggestions, interpreting scan results, proposing exploits, and report assistance. Does not execute tools, but tells you what to run next.                                                                            | Primarily OpenAI (GPT-4 recommended); can use local LLMs (GPT4All, etc.) with reduced performance. CLI tool on Python (cross-platform).                                                                | MIT License                                                                     | \~8k GitHub stars (very active); USENIX 2024 paper, strong community involvement and ongoing development.                                                                                                                   |
| **Nebula**        | Runs open-source LLMs locally (e.g. Llama 8B, Mistral 7B) for NLP. AI translates natural language into tool commands and analyzes tool output in real-time for insights.                                                                                      | Automating recon and analysis: executes scans (Nmap, ZAP, etc.) and uses AI to highlight vulns, suggest next steps, and log findings. Good for note-taking and orchestrating multi-tool workflows.                                                      | Open-source models via Ollama backend (runs offline). Ships with Meta Llama2-based and Mistral models tuned for instruct tasks. Cross-platform (Python, Docker optional).                              | BSD-2-Clause                                                                    | \~600+ stars (active as of 2025); growing user base, developer blog and community support. Regular updates, with a Pro (autonomous) version in development.                                                                 |
| **ChatAFL**       | Guides AFL-based fuzzer with GPT-3.5/4 intelligence. LLM infers protocol grammar, diversifies fuzz inputs, and suggests new states to explore when coverage stalls. Essentially an LLM-in-the-loop fuzzer.                                                    | Autonomous network protocol fuzzing: finds vulnerabilities in text-based protocol implementations (HTTP, FTP, etc.) by generating valid yet varied message sequences. Useful for discovering memory corruption or logic flaws in network services.      | OpenAI GPT (via API) used for generating inputs. Built on AFLNet fuzzing engine; distributed in Docker for Linux. Could be extended to other LLMs with coding effort.                                  | Apache-2.0                                                                      | \~300–350 stars (research-focused); NDSS 2024 artifact. Academic community interest with limited general-purpose usage. Active development by authors, but smaller contributor base.                                        |
| **DeepExploit**   | Uses Deep Reinforcement Learning (A3C algorithm) instead of an LLM. The AI “agent” learns the optimal sequence of scanning and exploitation actions through trial-and-error, improving its strategy over time.                                                | Automated penetration and post-exploitation: scans target network, identifies services/CVEs, selects and executes Metasploit exploits, and pivots to internal targets after compromise. Effectively an AI that can autonomously hack through a network. | No language model; uses neural networks for decision-making. Requires Metasploit and a training environment. Implemented in Python (TensorFlow for DRL) on Kali or similar.                            | *Likely* Apache-2.0 (open source on GitHub via 13o-bbr-bbq) – research project. | \~700 stars (original repo + forks); featured at Black Hat/DEF CON. Not frequently updated now; serves as a proof-of-concept for AI-driven exploits. Niche community of researchers continuing work (e.g. AutoPentest-DRL). |
| **GyoiThon**      | Utilizes machine learning (pattern recognition on web data) to identify running software and map to vulnerabilities. Combines string matching + trained ML on web scans for tech fingerprinting. Then uses that info to trigger relevant Metasploit exploits. | Web app recon and exploitation automation: crawls target, finds subdomains, detects CMS/framework versions, looks up known CVEs, and auto-exploits if possible. Also performs non-destructive vuln assessments (“health check” mode) for safe scans.    | No LLM; uses custom ML model for version/CVE identification. Runs on Python3 with Metasploit RPC. Integrates Google and Censys APIs for extended reconnaissance. Targeted at web servers (HTTP/HTTPS). | Apache-2.0                                                                      | \~770 stars on GitHub; presented in 2018 (Black Hat Asia Arsenal, etc.). Limited recent activity, but historically important as an early AI pentest tool. Still usable for quick web scans in engagements.                  |

Each of these tools shows a different way that generative AI or ML can assist offensive security. PentestGPT and Nebula act as intelligent copilots for human testers, Garak targets AI systems themselves, ChatAFL augments fuzzing, while DeepExploit and GyoiThon attempt full automation of attacks using AI. Together, they highlight how LLMs and other AI techniques are enabling more efficient and powerful red team operations in various domains of cybersecurity.

**Sources:**

1. Uproot Security – *PentestGPT: Guide for Pro Hackers*
2. Arxiv (Deng et al. 2023) – *PentestGPT: LLM-empowered Penetration Testing*
3. Beryllium Sec – *Nebula 2.0 Announcement*
4. Medevel – *AI Meets Cybersecurity: Top 10 AI Pentesting Tools*
5. NVIDIA Garak GitHub – *LLM Vulnerability Scanner*
6. ChatAFL GitHub README – *LLM-guided Protocol Fuzzing (NDSS 2024)*
7. Medium (Hellreys) – *DeepExploit and AI Pentesting Tools*
8. GyoiThon GitHub README – *Automated Web Pentest Tool using ML*
