# **LLM Architecture for Security Professionals**

This section focuses on the architecture of large language models (LLMs), providing foundational knowledge for security professionals. It covers the key components of LLMs, such as tokenization, self-attention, transformers, and more.

---

## **Structure**

The following topics are covered in this section:

### **2.1 History of LLMs**
- Explore the history and evolution of large language models, from early NLP models to modern transformer-based architectures.
- File: [2_1_history_of_llms.md](2_1_history_of_llms.md)

### **2.2 Tokenization**
- Understand how text is processed into tokens, the basic building blocks for LLMs.
- File: [2_2_tokenization.md](2_2_tokenization.md)

### **2.3 Self-Attention**
- Learn about the self-attention mechanism and its role in capturing contextual relationships in text.
- File: [2_3_self_attention.md](2_3_self_attention.md)

### **2.4 Transformer Architecture**
- Dive into the transformer architecture, which serves as the backbone for most modern LLMs.
- File: [2_4_transformer.md](2_4_transformer.md)

### **2.5 Hyperparameters**
- Understand the critical hyperparameters of LLMs and their impact on training and performance.
- File: [2_5_hyperparameters.md](2_5_hyperparameters.md)

### **2.6 LLM Comparisons**
- Compare different large language models to understand their strengths, weaknesses, and use cases.
- File: [2_6_llm_comparisons.md](2_6_llm_comparisons.md)

---

