# üß™ toy_nn ‚Äì FashionMNIST Classifier with Adversarial Attacks

This project is a minimal neural network training and adversarial attack framework for **FashionMNIST**.  
It supports two modes:

- **train** ‚Üí Train a simple feed-forward neural network on FashionMNIST and save a checkpoint.
- **attack** ‚Üí Load a trained checkpoint, perform **FGSM** and **PGD** adversarial attacks, and visualize misclassified examples.

---

## üöÄ Setup

We use [uv](https://github.com/astral-sh/uv) for dependency management.

**Lab Location> cd labs/ai-red-teaming-training/lab/ai_security/adversarial/nn/toy_nn**

```bash
# Create a virtual environment
uv venv .venv
source .venv/bin/activate

# Install dependencies
uv add torch torchvision matplotlib numpy
```

---

## üìÇ Project Structure

```
.
‚îú‚îÄ‚îÄ data/                 # Downloaded FashionMNIST dataset
‚îú‚îÄ‚îÄ runs/                 # Saved checkpoints & adversarial images
‚îú‚îÄ‚îÄ main.py               # Entry point (train / attack modes)
‚îú‚îÄ‚îÄ README.md             # This file
‚îú‚îÄ‚îÄ pyproject.toml        # uv project config
‚îî‚îÄ‚îÄ uv.lock               # dependency lockfile
```



## üß© Adversarial attacks

Think of the model like a very picky reader of images. **Adversarial attacks** are tiny, carefully chosen tweaks to a picture that a person wouldn‚Äôt notice, but that can **confuse the model** into the wrong answer.

* **‚ÄúTiny nudge‚Äù budget (Œµ):** We‚Äôre only allowed to make *very small* changes to each pixel‚Äîlike adjusting the brightness of a few dots by a nearly invisible amount. That‚Äôs our fairness rule: keep the image looking the same to humans.

* **FGSM (one hard shove):** We look at which tiny change would make the model most wrong, then take **one step** in that direction. It‚Äôs fast and often enough to flip a prediction.

* **PGD (many gentle pushes):** Instead of one step, we take **lots of small steps**, each time nudging the image a bit, then pulling it back if we overstep the ‚Äútiny nudge‚Äù limit. This is usually **stronger** than FGSM.

* **Why it works:** Models learn patterns that aren‚Äôt always the ones humans see. These microscopic tweaks exploit those hidden patterns and push the model off course.

* **Hardening (robust training):** We teach the model to **expect** these tiny tricks by training it on both clean images and slightly perturbed/noisy ones. Over time, it learns to **ignore** the tricks and stay correct.

![Adversarial triptych](examples/adv_triptych.png)


## üß† Attack Details

### Threat model (L‚àû, pixel space)

We add a small perturbation $\delta$ to an input image $x \in [0,1]^{1\times 28\times 28}$ so that the model‚Äôs prediction changes, while keeping $\|\delta\|_\infty \le \varepsilon$ (max per-pixel change ‚â§ Œµ).
We always **clamp** to the valid image range and **project** back to the Œµ-ball so the perturbation budget isn‚Äôt exceeded.

* **Clamp:** `x = torch.clamp(x, 0.0, 1.0)`
* **Project (L‚àû):** `x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)`

This repo keeps data in **\[0,1]** (no normalization), so **Œµ and Œ± are in pixel units** (e.g., `8/255 ‚âà 0.031`).

---

### FGSM (Fast Gradient Sign Method, one-step)

Untargeted FGSM takes **one step** in the direction that *increases* loss:

$$
x_{\text{adv}} = \text{clip}_{[0,1]}\Big(x + \varepsilon \cdot \text{sign}\big(\nabla_x \mathcal{L}(f(x), y)\big)\Big)
$$

* In code: `fgsm_attack(...)`
* Steps:

  1. `x.requires_grad_(True)`, forward pass
  2. `loss = NLLLoss(log_probs, y)`; `loss.backward()` to get `x.grad`
  3. Add `eps * sign(x.grad)`, then clamp to `[0,1]`

**Targeted FGSM** (not enabled by default) flips the sign to *decrease* loss toward a chosen target class $t$:
`x_adv = x - eps * sign(‚àá_x L(f(x), t))`

---

### PGD (Projected Gradient Descent, multi-step)

PGD is an **iterative** version of FGSM with a per-step size $\alpha$ and projection back into the Œµ-ball each iteration. We also start from a **random point** inside the Œµ-ball (‚Äúrandom start‚Äù) to avoid weak local optima.

Repeat for `steps`:

$$
x^{k+1} = \Pi_{B_\infty(x,\varepsilon)}\Big( \text{clip}_{[0,1]}(x^k + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(f(x^k), y))) \Big)
$$

* In code: `pgd_attack(...)`
* Key lines:

  * **Random start:** `delta ~ U(-eps, eps)`, `x_adv = clamp(x + delta)`
  * **Update:** `x_adv += alpha * sign(grad)`
  * **Project:** `x_adv = max(min(x_adv, x + eps), x - eps)`
  * **Clamp to image range:** `clamp(0,1)`

---

### Choosing Œµ (budget) and Œ± (step)

* Common Œµ (L‚àû): **`4/255`, `8/255`, `16/255`, `32/255`**
* Rule of thumb for PGD: **Œ± ‚âà Œµ/4 ‚Ä¶ Œµ/2**, **steps = 10‚Äì40**
* In CLI:

  * `--eps 8,16,32` (interpreted as `/255`), or decimals `--eps 0.031,0.063`
  * `--pgd-alpha auto` picks `max(eps/4, 1/255)`


## üéì Training

Train a model for 5 epochs with batch size 64 and save to `./runs/fashion_mnist.pth`:

```bash
python main.py --mode train \
    --epochs 5 \
    --batch-size 64 \
    --ckpt ./runs/fashion_mnist.pth
```

Example output:

```
Epoch 5
-------------------------------
loss: 0.272584 [51200/60000]
Test accuracy: 0.872
Training done!
Saved model to: ./runs/fashion_mnist.pth
```

---

## ‚öîÔ∏è Adversarial Attack

Run FGSM + PGD attacks against the trained model:

```bash
python main.py --mode attack \
    --ckpt ./runs/fashion_mnist.pth \
    --eps 0.1255 \
    --pgd-steps 20 \
    --pgd-alpha 0.0314 \
    --visualize \
    --no-show \
    --save-dir ./runs
```

This will:

* Print clean accuracy and adversarial accuracy
* Save misclassified adversarial examples:

  * `adv_triptych.png` (clean vs adv vs perturbation)
  * `misclassified_adv_epsXX.png` (single adversarial example)

---

### üìä Example Results

```
[ATTACK] Loaded ckpt: ./runs/fashion_mnist.pth (meta={'clean_acc': 0.8715, 'epochs': 5}). Clean acc: 0.872

[FGSM] accuracy:
  eps=0.1255: 0.018

[PGD] accuracy:
  eps=0.1255, alpha=0.0314, steps=20: 0.004

Saved triptych to: ./runs/adv_triptych.png
Saved adversarial image to: ./runs/misclassified_adv_eps32.png
```
---

## üõ°Ô∏è Harden (Adversarial + Noise Training)

Fine-tune a saved model with a mix of **PGD adversarial examples** and **noisy inputs** (Gaussian & salt-and-pepper), then save a hardened checkpoint.

```bash
python main.py --mode harden \
  --ckpt ./runs/fashion_mnist.pth \
  --ckpt-out ./runs/fashion_mnist_hardened.pth \
  --harden-epochs 3 \
  --harden-lr 5e-4 \
  --adv-frac 0.5 \
  --noise-frac 0.3 \
  --harden-eps 8,16 \
  --harden-steps 5 \
  --harden-alpha auto \
  --noise-gauss-std 0.05 \
  --noise-sp-prob 0.02
```

* `adv-frac` = fraction of each batch converted to **PGD adversarial** examples
* `noise-frac` = fraction converted to **noisy** examples (rest stays clean)
* `harden-eps` uses per-255 shortcuts (`8,16` ‚Üí `8/255,16/255`); pass decimals to use raw values
* Output is saved to `--ckpt-out` (keep your original checkpoint intact)

---

## üî¨ Compare (Baseline vs. Hardened)

Run a **side-by-side** robustness comparison (Clean, FGSM@Œµ, PGD@Œµ) of two checkpoints:

```bash
python main.py --mode compare \
  --ckpt-a ./runs/fashion_mnist.pth \
  --ckpt-b ./runs/fashion_mnist_hardened.pth \
  --eps 8,16,32 \
  --pgd-steps 20 \
  --pgd-alpha auto
```

You‚Äôll get a compact table showing metrics for A and B and the Œî(B‚àíA).

---

## üßæ Commands quick reference

**Train**

```bash
python main.py --mode train \
  --epochs 5 --batch-size 64 --lr 1e-3 \
  --ckpt ./runs/fashion_mnist.pth
```

**Attack (FGSM + PGD)**

```bash
python main.py --mode attack \
  --ckpt ./runs/fashion_mnist.pth \
  --eps 8,16,32 --pgd-steps 20 --pgd-alpha auto \
  --visualize --no-show --save-dir ./runs
```

**Harden**

```bash
python main.py --mode harden \
  --ckpt ./runs/fashion_mnist.pth \
  --ckpt-out ./runs/fashion_mnist_hardened.pth \
  --harden-epochs 3 --adv-frac 0.5 --noise-frac 0.3 \
  --harden-eps 8,16 --harden-steps 5 --harden-alpha auto
```

**Compare**

```bash
python main.py --mode compare \
  --ckpt-a ./runs/fashion_mnist.pth \
  --ckpt-b ./runs/fashion_mnist_hardened.pth \
  --eps 8,16,32 --pgd-steps 20 --pgd-alpha auto
```

---

## üß∞ Useful flags

* `--eps 8,16,32` ‚Üí interpreted as **/255** (use decimals like `0.1255` for raw values)
* `--pgd-alpha auto` ‚Üí uses `max(eps/4, 1/255)`
* `--no-show` ‚Üí save figures without opening a window (headless servers)
* `--no-cuda` ‚Üí force CPU even if CUDA is available
* `--seed 42 --deterministic` ‚Üí reproducible runs


---

## üñºÔ∏è Visualization

* **Clean** vs **Adversarial** vs **Perturbation** images are saved in `./runs/`
* Perturbation is scaled for visibility

---

## ‚ö†Ô∏è Disclaimer

This is a **toy project** for research & educational purposes only.
Do not use adversarial attack code for malicious purposes.

